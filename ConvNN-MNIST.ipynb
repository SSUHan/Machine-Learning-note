{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_instance_num, pixel_num = mnist.train.images.shape\n",
    "test_instance_num, class_num = mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', shape=[None, pixel_num])\n",
    "y_answer = tf.placeholder('float', shape=[None, class_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1]) # 입력데이터를 원래 이미지의 구조로 재구성한다.\n",
    "x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필터용 W를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "여기에서 초기 가중치를 임의잡음(random noise) 으로 잡은 것은, 초기화 값에 따라서 서로 다른 특징들을 뽑아내길 기대하기 때문으로 볼 수 있다.\n",
    "\"\"\"\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필터용 bias 를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱 계층을 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맥스 풀링 계층 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫번째 합성곱 계층 과 풀링 계층을 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5,5,1,32]) # 5*5 크기의 1채널을 상대하는 32개 의 필터를 생성\n",
    "b_conv1 = bias_variable([32]) # 32 개의 필터에 들어갈 편향값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # relu 로 activation function 을 만들어 Hidden Layer 를 구성\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두번째 합성곱 계층 과 풀링 계층 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5,5,32,64]) # 5*5 크기의 32 채널 상대, 64개의 필터를 생성\n",
    "b_conv2 = bias_variable([64]) # 64개의 필터에 들어갈 편향값들\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # 여기서의 인풋은 1단계 Hidden Layer 의 결과값이 들어가게 됨\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소프트맥스 계층에 주입하기 위해서 FC 계층을 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7*7*64, 1024]) # 7*7 짜리 pool 결과가 64개가 있고, 이 것들을 전부 연결하고, 이러한 뉴런의 개수가 1024 개로 정의\n",
    "b_fc1 = bias_variable([1024]) # 1024 개의 뉴런의 편향값\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) # 1자 벡터화\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  # 완전 연결 계층 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "신경망에서 필요한 매개변수 수를 줄이는 것\n",
    "노드를 삭제하여 입력과 출력 사이의 연결을 제거하는 것\n",
    "어떤 뉴런을 제거하고, 어떤 뉴런을 유지할 지는 무작위로 결정\n",
    "뉴런이 제거되거나 그렇지 않을 확률은 코드로 처리하지 않고 텐서플로에게 위임\n",
    "드롭아웃은 모델이 데이터에 오버피팅 되는 현상을 막아주는 효과\n",
    "오버피팅이란 입력 데이터의 차원에 비해서 더 많은 매개변수를 가지는 모델에서 자주 발생하는 현상\n",
    "\"\"\"\n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소프트맥스 계층에 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, class_num]) # 1024 개의 뉴런의 결과를 10개의 뉴런으로 압축처리 -> 10개의 클래스로 결정\n",
    "b_fc2 = bias_variable([class_num]) # 10 개의 클래스의 편향값\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) # 10 개의 클래스로 결과가 softmax 함수값으로 결정 될 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = - tf.reduce_sum(y_answer*tf.log(y_conv)) # 크로스 엔트로피 방식으로 오차값을 선정\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_answer, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy : 0.06\n",
      "step 100, training accuracy : 0.9\n",
      "step 200, training accuracy : 0.93\n",
      "step 300, training accuracy : 0.96\n",
      "step 400, training accuracy : 0.92\n",
      "step 500, training accuracy : 0.94\n",
      "step 600, training accuracy : 0.96\n",
      "step 700, training accuracy : 0.92\n",
      "step 800, training accuracy : 0.9\n",
      "step 900, training accuracy : 0.95\n",
      "===최종학습 결과===\n",
      "test accuracy : 0.9685\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100) # 100 개씩 랜덤하게 픽해서 가져와 학습\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x:batch[0], y_answer:batch[1], keep_prob:1.0})\n",
    "        print(\"step %d, training accuracy : %g\" % (i, train_accuracy))\n",
    "    sess.run(train_step, feed_dict={x:batch[0], y_answer:batch[1], keep_prob:0.5})\n",
    "\n",
    "print(\"===최종학습 결과===\")\n",
    "print(\"test accuracy : %g\" % sess.run(accuracy, feed_dict={x:mnist.test.images, y_answer:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
